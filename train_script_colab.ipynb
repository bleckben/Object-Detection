{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# Car Detection with YOLOv11 and YOLOv12 - Google Colab Version\n",
    "\n",
    "**Dataset:** Car Detection Dataset (BMW-X5-M, Volvo-XC40, Jaguar)\n",
    "\n",
    "**Models:** YOLOv11 and YOLOv12\n",
    "\n",
    "**Environment:** Google Colab\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup-environment"
   },
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install-packages"
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "print(\"Installing required packages...\")\n",
    "!pip install -q ultralytics roboflow\n",
    "!pip install -q opencv-python-headless\n",
    "!pip install -q matplotlib\n",
    "!pip install -q numpy\n",
    "!pip install -q seaborn\n",
    "!pip install -q pandas\n",
    "print(\"\\n Packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mount-drive"
   },
   "outputs": [],
   "source": [
    "# Mount Google Drive (optional - for saving results)\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "print(\"\\n Google Drive mounted at /content/drive\")\n",
    "\n",
    "# Create results directory in Drive\n",
    "import os\n",
    "DRIVE_RESULTS = '/content/drive/MyDrive/YOLO_Car_Detection_Results'\n",
    "os.makedirs(DRIVE_RESULTS, exist_ok=True)\n",
    "print(f\" Results will be saved to: {DRIVE_RESULTS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "system-info"
   },
   "outputs": [],
   "source": [
    "# Check Python and system information\n",
    "import sys\n",
    "import platform\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"SYSTEM INFORMATION\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Platform: {platform.system()} {platform.release()}\")\n",
    "print(f\"Machine: {platform.machine()}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import-libraries"
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import time\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "from collections import Counter\n",
    "import json\n",
    "from PIL import Image\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Deep Learning libraries\n",
    "from ultralytics import YOLO\n",
    "import torch\n",
    "\n",
    "# Set visualization style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# For better plot display in Colab\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LIBRARY VERSIONS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"OpenCV version: {cv2.__version__}\")\n",
    "print(f\"Matplotlib version: {plt.matplotlib.__version__}\")\n",
    "print(f\"Seaborn version: {sns.__version__}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Check GPU availability\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"GPU INFORMATION\")\n",
    "print(\"=\"*80)\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device count: {torch.cuda.device_count()}\")\n",
    "    print(f\"CUDA device name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"cuDNN version: {torch.backends.cudnn.version()}\")\n",
    "else:\n",
    "    print(\"  No GPU detected - Training will use CPU (slower)\")\n",
    "    print(\"   Please enable GPU: Runtime > Change runtime type > GPU\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "start-timer"
   },
   "outputs": [],
   "source": [
    "# Record start time for entire execution\n",
    "execution_start_time = time.time()\n",
    "print(f\"Execution started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dataset-section"
   },
   "source": [
    "## Dataset Loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dataset-config"
   },
   "outputs": [],
   "source": [
    "# Configuration\n",
    "DATASET_DIR = \"/content/dataset\" \n",
    "RESULTS_DIR = \"/content/results\"\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Dataset directory: {DATASET_DIR}\")\n",
    "print(f\"Results directory: {RESULTS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "verify-dataset"
   },
   "outputs": [],
   "source": [
    "# Verify dataset structure\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"VERIFYING DATASET STRUCTURE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "dataset_path = DATASET_DIR\n",
    "\n",
    "# Required paths\n",
    "required_paths = [\n",
    "    f'{dataset_path}/data.yaml',\n",
    "    f'{dataset_path}/train/images',\n",
    "    f'{dataset_path}/train/labels',\n",
    "    f'{dataset_path}/valid/images',\n",
    "    f'{dataset_path}/valid/labels'\n",
    "]\n",
    "\n",
    "all_paths_exist = True\n",
    "for path in required_paths:\n",
    "    if os.path.exists(path):\n",
    "        print(f\"Found: {path}\")\n",
    "    else:\n",
    "        print(f\"Missing: {path}\")\n",
    "        all_paths_exist = False\n",
    "\n",
    "if not all_paths_exist:\n",
    "    raise FileNotFoundError(\"Required dataset structure not found!\")\n",
    "    \n",
    "print(\"\\n Dataset structure verified!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load-dataset-config"
   },
   "outputs": [],
   "source": [
    "# Load dataset configuration\n",
    "with open(f'{dataset_path}/data.yaml', 'r') as f:\n",
    "    data_config = yaml.safe_load(f)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DATASET CONFIGURATION\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Number of classes: {data_config['nc']}\")\n",
    "print(f\"\\nClass names:\")\n",
    "for i, name in enumerate(data_config['names']):\n",
    "    print(f\"  {i}: {name}\")\n",
    "print(f\"\\nTrain path: {data_config.get('train', 'Not specified')}\")\n",
    "print(f\"Validation path: {data_config.get('val', data_config.get('valid', 'Not specified'))}\")\n",
    "print(f\"Test path: {data_config.get('test', 'Not provided')}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eda-section"
   },
   "source": [
    "## Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eda-functions"
   },
   "outputs": [],
   "source": [
    "def analyze_dataset_split(base_path, split='train'):\n",
    "    \"\"\"Analyze a dataset split and return statistics\"\"\"\n",
    "    images_path = Path(base_path) / split / 'images'\n",
    "    labels_path = Path(base_path) / split / 'labels'\n",
    "    \n",
    "    # Count images and labels\n",
    "    images = list(images_path.glob('*.jpg')) + list(images_path.glob('*.png'))\n",
    "    labels = list(labels_path.glob('*.txt'))\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"{split.upper()} SET STATISTICS\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Number of images: {len(images)}\")\n",
    "    print(f\"Number of labels: {len(labels)}\")\n",
    "    \n",
    "    # Analyze image dimensions (sample)\n",
    "    image_dims = []\n",
    "    for img_path in images[:100]:\n",
    "        img = Image.open(img_path)\n",
    "        image_dims.append(img.size)\n",
    "    \n",
    "    if image_dims:\n",
    "        widths, heights = zip(*image_dims)\n",
    "        print(f\"\\nImage Dimensions (sample of {len(image_dims)}):\")\n",
    "        print(f\"  Width  - Min: {min(widths):4d}, Max: {max(widths):4d}, Mean: {np.mean(widths):6.1f}\")\n",
    "        print(f\"  Height - Min: {min(heights):4d}, Max: {max(heights):4d}, Mean: {np.mean(heights):6.1f}\")\n",
    "    \n",
    "    # Analyze class distribution\n",
    "    class_counts = Counter()\n",
    "    bbox_counts = []\n",
    "    bbox_areas = []\n",
    "    \n",
    "    for label_file in labels:\n",
    "        with open(label_file, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            bbox_counts.append(len(lines))\n",
    "            for line in lines:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) >= 5:\n",
    "                    class_id = int(parts[0])\n",
    "                    class_counts[class_id] += 1\n",
    "                    \n",
    "                    # Get bbox area\n",
    "                    width, height = float(parts[3]), float(parts[4])\n",
    "                    bbox_areas.append(width * height)\n",
    "    \n",
    "    if bbox_counts:\n",
    "        print(f\"\\nBounding Box Statistics:\")\n",
    "        print(f\"  Total boxes: {sum(bbox_counts)}\")\n",
    "        print(f\"  Avg boxes per image: {np.mean(bbox_counts):.2f}\")\n",
    "        print(f\"  Min/Max boxes: {min(bbox_counts)}/{max(bbox_counts)}\")\n",
    "    \n",
    "    return {\n",
    "        'class_counts': class_counts,\n",
    "        'bbox_counts': bbox_counts,\n",
    "        'image_dims': image_dims,\n",
    "        'bbox_areas': bbox_areas,\n",
    "        'images': images,\n",
    "        'labels': labels\n",
    "    }\n",
    "\n",
    "# Analyze all splits\n",
    "train_stats = analyze_dataset_split(dataset_path, 'train')\n",
    "val_stats = analyze_dataset_split(dataset_path, 'valid')\n",
    "\n",
    "if os.path.exists(Path(dataset_path) / 'test'):\n",
    "    test_stats = analyze_dataset_split(dataset_path, 'test')\n",
    "else:\n",
    "    test_stats = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "visualize-class-distribution"
   },
   "outputs": [],
   "source": [
    "# Visualize class distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Training set class distribution\n",
    "classes = [data_config['names'][i] for i in sorted(train_stats['class_counts'].keys())]\n",
    "counts = [train_stats['class_counts'][i] for i in sorted(train_stats['class_counts'].keys())]\n",
    "\n",
    "axes[0].bar(range(len(classes)), counts, color='steelblue', alpha=0.8, edgecolor='black')\n",
    "axes[0].set_xlabel('Class', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Number of Instances', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('Class Distribution - Training Set', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xticks(range(len(classes)))\n",
    "axes[0].set_xticklabels(classes, rotation=45, ha='right')\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Objects per image distribution\n",
    "axes[1].hist(train_stats['bbox_counts'], bins=30, color='coral', alpha=0.8, edgecolor='black')\n",
    "axes[1].set_xlabel('Number of Objects per Image', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Frequency', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title('Objects per Image Distribution', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{RESULTS_DIR}/eda_class_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n Saved: {RESULTS_DIR}/eda_class_distribution.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "visualize-samples"
   },
   "outputs": [],
   "source": [
    "# Visualize sample images with annotations\n",
    "def visualize_samples(num_samples=9):\n",
    "    train_images_path = Path(dataset_path) / 'train' / 'images'\n",
    "    train_labels_path = Path(dataset_path) / 'train' / 'labels'\n",
    "    \n",
    "    sample_images = np.random.choice(train_stats['images'], \n",
    "                                     min(num_samples, len(train_stats['images'])), \n",
    "                                     replace=False)\n",
    "    \n",
    "    grid_size = int(np.ceil(np.sqrt(num_samples)))\n",
    "    fig, axes = plt.subplots(grid_size, grid_size, figsize=(20, 20))\n",
    "    axes = axes.ravel() if num_samples > 1 else [axes]\n",
    "    \n",
    "    for idx, img_path in enumerate(sample_images):\n",
    "        # Read image\n",
    "        img = cv2.imread(str(img_path))\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        h, w = img.shape[:2]\n",
    "        \n",
    "        # Read labels\n",
    "        label_path = train_labels_path / f\"{img_path.stem}.txt\"\n",
    "        num_boxes = 0\n",
    "        \n",
    "        if label_path.exists():\n",
    "            with open(label_path, 'r') as f:\n",
    "                for line in f.readlines():\n",
    "                    parts = line.strip().split()\n",
    "                    if len(parts) >= 5:\n",
    "                        class_id = int(parts[0])\n",
    "                        x_center, y_center, width, height = map(float, parts[1:5])\n",
    "                        \n",
    "                        # Convert to pixel coordinates\n",
    "                        x1 = int((x_center - width/2) * w)\n",
    "                        y1 = int((y_center - height/2) * h)\n",
    "                        x2 = int((x_center + width/2) * w)\n",
    "                        y2 = int((y_center + height/2) * h)\n",
    "                        \n",
    "                        # Draw box\n",
    "                        cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                        \n",
    "                        # Add label\n",
    "                        class_name = data_config['names'][class_id]\n",
    "                        cv2.putText(img, class_name, (x1, y1-10),\n",
    "                                  cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "                        num_boxes += 1\n",
    "        \n",
    "        axes[idx].imshow(img)\n",
    "        axes[idx].axis('off')\n",
    "        axes[idx].set_title(f\"Sample {idx+1} ({num_boxes} objects)\", fontsize=10, fontweight='bold')\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for idx in range(len(sample_images), len(axes)):\n",
    "        axes[idx].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{RESULTS_DIR}/sample_annotated_images.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"\\n Saved: {RESULTS_DIR}/sample_annotated_images.png\")\n",
    "\n",
    "visualize_samples(9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "training-yolov11"
   },
   "source": [
    "## Model Training - YOLOv11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "training-config"
   },
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "EPOCHS = 50\n",
    "IMG_SIZE = 640\n",
    "BATCH_SIZE = 16\n",
    "DEVICE = 0 if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print(f\"Training Configuration:\")\n",
    "print(f\"Epochs: {EPOCHS}\")\n",
    "print(f\"Image Size: {IMG_SIZE}\")\n",
    "print(f\"Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"Device: {'GPU' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train-yolov11"
   },
   "outputs": [],
   "source": [
    "# Train YOLOv11\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING YOLOv11\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "yolov11_start_time = time.time()\n",
    "\n",
    "# Initialize model\n",
    "print(\"\\nInitializing YOLOv11 model...\")\n",
    "model_v11 = YOLO('yolo11n.pt')  # Will auto-download if not present\n",
    "\n",
    "# Train\n",
    "print(\"\\nStarting training...\")\n",
    "results_v11 = model_v11.train(\n",
    "    data=f'{dataset_path}/data.yaml',\n",
    "    epochs=EPOCHS,\n",
    "    imgsz=IMG_SIZE,\n",
    "    batch=BATCH_SIZE,\n",
    "    name='yolov11_car_detection',\n",
    "    project='/content/runs/detect',\n",
    "    patience=10,\n",
    "    save=True,\n",
    "    device=DEVICE,\n",
    "    optimizer='AdamW',\n",
    "    lr0=0.01,\n",
    "    lrf=0.01,\n",
    "    momentum=0.937,\n",
    "    weight_decay=0.0005,\n",
    "    warmup_epochs=3,\n",
    "    warmup_momentum=0.8,\n",
    "    box=7.5,\n",
    "    cls=0.5,\n",
    "    dfl=1.5,\n",
    "    plots=True,\n",
    "    verbose=True,\n",
    "    workers=2  # Reduced for Colab stability\n",
    ")\n",
    "\n",
    "yolov11_training_time = time.time() - yolov11_start_time\n",
    "\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(f\"YOLOv11 Training completed in {yolov11_training_time:.2f} seconds ({yolov11_training_time/60:.2f} minutes)\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "validate-yolov11"
   },
   "outputs": [],
   "source": [
    "# Validate YOLOv11\n",
    "print(\"\\nValidating YOLOv11 model...\")\n",
    "val_results_v11 = model_v11.val(data=f'{dataset_path}/data.yaml', workers=2)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"YOLOv11 VALIDATION METRICS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"mAP@0.5      : {val_results_v11.box.map50:.4f}\")\n",
    "print(f\"mAP@0.5:0.95 : {val_results_v11.box.map:.4f}\")\n",
    "print(f\"Precision    : {val_results_v11.box.mp:.4f}\")\n",
    "print(f\"Recall       : {val_results_v11.box.mr:.4f}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "training-yolov12"
   },
   "source": [
    "## Model Training - YOLOv12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train-yolov12"
   },
   "outputs": [],
   "source": [
    "# Train YOLOv12\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING YOLOv12\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "yolov12_start_time = time.time()\n",
    "\n",
    "# Initialize model\n",
    "print(\"\\nInitializing YOLOv12 model...\")\n",
    "model_v12 = YOLO('yolo12n.pt')  # Will auto-download if not present\n",
    "\n",
    "# Train\n",
    "print(\"\\nStarting training...\")\n",
    "results_v12 = model_v12.train(\n",
    "    data=f'{dataset_path}/data.yaml',\n",
    "    epochs=EPOCHS,\n",
    "    imgsz=IMG_SIZE,\n",
    "    batch=BATCH_SIZE,\n",
    "    name='yolov12_car_detection',\n",
    "    project='/content/runs/detect',\n",
    "    patience=10,\n",
    "    save=True,\n",
    "    device=DEVICE,\n",
    "    optimizer='AdamW',\n",
    "    lr0=0.01,\n",
    "    lrf=0.01,\n",
    "    momentum=0.937,\n",
    "    weight_decay=0.0005,\n",
    "    warmup_epochs=3,\n",
    "    warmup_momentum=0.8,\n",
    "    box=7.5,\n",
    "    cls=0.5,\n",
    "    dfl=1.5,\n",
    "    plots=True,\n",
    "    verbose=True,\n",
    "    workers=2\n",
    ")\n",
    "\n",
    "yolov12_training_time = time.time() - yolov12_start_time\n",
    "\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(f\"YOLOv12 Training completed in {yolov12_training_time:.2f} seconds ({yolov12_training_time/60:.2f} minutes)\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "validate-yolov12"
   },
   "outputs": [],
   "source": [
    "# Validate YOLOv12\n",
    "print(\"\\nValidating YOLOv12 model...\")\n",
    "val_results_v12 = model_v12.val(data=f'{dataset_path}/data.yaml', workers=2)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"YOLOv12 VALIDATION METRICS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"mAP@0.5      : {val_results_v12.box.map50:.4f}\")\n",
    "print(f\"mAP@0.5:0.95 : {val_results_v12.box.map:.4f}\")\n",
    "print(f\"Precision    : {val_results_v12.box.mp:.4f}\")\n",
    "print(f\"Recall       : {val_results_v12.box.mr:.4f}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "performance-comparison"
   },
   "source": [
    "## Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create-comparison"
   },
   "outputs": [],
   "source": [
    "# Create comparison dataframe\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': ['YOLOv11', 'YOLOv12'],\n",
    "    'mAP@0.5': [val_results_v11.box.map50, val_results_v12.box.map50],\n",
    "    'mAP@0.5:0.95': [val_results_v11.box.map, val_results_v12.box.map],\n",
    "    'Precision': [val_results_v11.box.mp, val_results_v12.box.mp],\n",
    "    'Recall': [val_results_v11.box.mr, val_results_v12.box.mr],\n",
    "    'Training Time (min)': [yolov11_training_time/60, yolov12_training_time/60]})\n",
    "\n",
    "# Calculate F1 score\n",
    "comparison_df['F1 Score'] = 2 * (comparison_df['Precision'] * comparison_df['Recall']) / \\\n",
    "                            (comparison_df['Precision'] + comparison_df['Recall'])\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PERFORMANCE COMPARISON: YOLOv11 vs YOLOv12\")\n",
    "print(\"=\"*80)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Save comparison\n",
    "comparison_df.to_csv(f'{RESULTS_DIR}/model_comparison_colab.csv', index=False)\n",
    "print(f\"\\n Saved: {RESULTS_DIR}/model_comparison_colab.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "visualize-comparison"
   },
   "outputs": [],
   "source": [
    "# Visualize performance comparison\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "metrics = ['mAP@0.5', 'mAP@0.5:0.95', 'Precision', 'Recall', 'F1 Score']\n",
    "colors = ['#3498db', '#e74c3c']\n",
    "\n",
    "for idx, metric in enumerate(metrics):\n",
    "    ax = axes[idx]\n",
    "    values = comparison_df[metric].values\n",
    "    bars = ax.bar(comparison_df['Model'], values, color=colors, alpha=0.8, \n",
    "                  edgecolor='black', linewidth=2)\n",
    "    \n",
    "    ax.set_ylabel(metric, fontsize=12, fontweight='bold')\n",
    "    ax.set_title(f'{metric} Comparison', fontsize=14, fontweight='bold')\n",
    "    ax.set_ylim([0, 1.0])\n",
    "    ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height + 0.02,\n",
    "               f'{height:.4f}',\n",
    "               ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "# Training time comparison\n",
    "ax = axes[5]\n",
    "training_times = comparison_df['Training Time (min)'].values\n",
    "bars = ax.bar(comparison_df['Model'], training_times, color=colors, alpha=0.8,\n",
    "             edgecolor='black', linewidth=2)\n",
    "\n",
    "ax.set_ylabel('Training Time (minutes)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Training Time Comparison', fontsize=14, fontweight='bold')\n",
    "ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "           f'{height:.2f} min',\n",
    "           ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{RESULTS_DIR}/performance_comparison_colab.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n Saved: {RESULTS_DIR}/performance_comparison_colab.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "evaluation-section"
   },
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "display-confusion-matrices"
   },
   "outputs": [],
   "source": [
    "# Display confusion matrices\n",
    "import glob\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 8))\n",
    "\n",
    "# YOLOv11 confusion matrix\n",
    "v11_runs = sorted(glob.glob('/content/runs/detect/yolov11_*'), key=os.path.getmtime, reverse=True)\n",
    "v11_cm_path = None\n",
    "if v11_runs:\n",
    "    for run_dir in v11_runs:\n",
    "        cm_path = os.path.join(run_dir, 'confusion_matrix.png')\n",
    "        if os.path.exists(cm_path):\n",
    "            v11_cm_path = cm_path\n",
    "            break\n",
    "\n",
    "if v11_cm_path and os.path.exists(v11_cm_path):\n",
    "    img_v11_cm = plt.imread(v11_cm_path)\n",
    "    axes[0].imshow(img_v11_cm)\n",
    "    axes[0].set_title('YOLOv11 Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "    axes[0].axis('off')\n",
    "else:\n",
    "    axes[0].text(0.5, 0.5, 'Confusion matrix not found', \n",
    "                ha='center', va='center', transform=axes[0].transAxes, fontsize=12)\n",
    "    axes[0].set_title('YOLOv11 Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "\n",
    "# YOLOv12 confusion matrix\n",
    "v12_runs = sorted(glob.glob('/content/runs/detect/yolov12_*'), key=os.path.getmtime, reverse=True)\n",
    "v12_cm_path = None\n",
    "if v12_runs:\n",
    "    for run_dir in v12_runs:\n",
    "        cm_path = os.path.join(run_dir, 'confusion_matrix.png')\n",
    "        if os.path.exists(cm_path):\n",
    "            v12_cm_path = cm_path\n",
    "            break\n",
    "\n",
    "if v12_cm_path and os.path.exists(v12_cm_path):\n",
    "    img_v12_cm = plt.imread(v12_cm_path)\n",
    "    axes[1].imshow(img_v12_cm)\n",
    "    axes[1].set_title('YOLOv12 Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "    axes[1].axis('off')\n",
    "else:\n",
    "    axes[1].text(0.5, 0.5, 'Confusion matrix not found', \n",
    "                ha='center', va='center', transform=axes[1].transAxes, fontsize=12)\n",
    "    axes[1].set_title('YOLOv12 Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{RESULTS_DIR}/confusion_matrices_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n Saved: {RESULTS_DIR}/confusion_matrices_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "inference-section"
   },
   "source": [
    "## Inference and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run-inference"
   },
   "outputs": [],
   "source": [
    "# Run inference on sample validation images\n",
    "val_images_path = Path(dataset_path) / 'valid' / 'images'\n",
    "val_images = list(val_images_path.glob('*.jpg')) + list(val_images_path.glob('*.png'))\n",
    "sample_test_images = np.random.choice(val_images, min(3, len(val_images)), replace=False)\n",
    "\n",
    "for idx, img_path in enumerate(sample_test_images):\n",
    "    print(f\"\\nProcessing: {img_path.name}\")\n",
    "    \n",
    "    # Get predictions\n",
    "    results_v11_inf = model_v11.predict(str(img_path), conf=0.25, save=False, verbose=False)\n",
    "    results_v12_inf = model_v12.predict(str(img_path), conf=0.25, save=False, verbose=False)\n",
    "    \n",
    "    # Visualize\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(20, 7))\n",
    "    \n",
    "    # Original\n",
    "    original_img = cv2.imread(str(img_path))\n",
    "    original_img = cv2.cvtColor(original_img, cv2.COLOR_BGR2RGB)\n",
    "    axes[0].imshow(original_img)\n",
    "    axes[0].set_title('Original Image', fontsize=13, fontweight='bold')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # YOLOv11\n",
    "    img_v11 = results_v11_inf[0].plot()\n",
    "    img_v11 = cv2.cvtColor(img_v11, cv2.COLOR_BGR2RGB)\n",
    "    axes[1].imshow(img_v11)\n",
    "    axes[1].set_title(f'YOLOv11 ({len(results_v11_inf[0].boxes)} detections)',\n",
    "                     fontsize=13, fontweight='bold', color='#3498db')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    # YOLOv12\n",
    "    img_v12 = results_v12_inf[0].plot()\n",
    "    img_v12 = cv2.cvtColor(img_v12, cv2.COLOR_BGR2RGB)\n",
    "    axes[2].imshow(img_v12)\n",
    "    axes[2].set_title(f'YOLOv12 ({len(results_v12_inf[0].boxes)} detections)',\n",
    "                     fontsize=13, fontweight='bold', color='#e74c3c')\n",
    "    axes[2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{RESULTS_DIR}/inference_comparison_{idx+1}.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\" Saved: {RESULTS_DIR}/inference_comparison_{idx+1}.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "summary-section"
   },
   "source": [
    "## Summary and Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "generate-summary"
   },
   "outputs": [],
   "source": [
    "# Calculate total execution time\n",
    "total_execution_time = time.time() - execution_start_time\n",
    "\n",
    "# Create comprehensive summary\n",
    "summary = f\"\"\"\n",
    "{'='*80}\n",
    "CAR DETECTION - COMPREHENSIVE SUMMARY (GOOGLE COLAB)\n",
    "{'='*80}\n",
    "\n",
    "DATASET INFORMATION:\n",
    "  Name: Car Detection Dataset\n",
    "  Number of Classes: {data_config['nc']}\n",
    "  Classes: {', '.join([str(c) for c in data_config['names']])}\n",
    "  Training Images: {len(train_stats['images'])}\n",
    "  Validation Images: {len(val_stats['images'])}\n",
    "\n",
    "COMPUTING ENVIRONMENT:\n",
    "  Platform: Google Colab\n",
    "  Device: {'GPU - ' + torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\n",
    "  PyTorch Version: {torch.__version__}\n",
    "  CUDA Available: {torch.cuda.is_available()}\n",
    "{('  CUDA Version: ' + str(torch.version.cuda)) if torch.cuda.is_available() else ''}\n",
    "\n",
    "YOLOV11 PERFORMANCE:\n",
    "  mAP@0.5      : {val_results_v11.box.map50:.4f}\n",
    "  mAP@0.5:0.95 : {val_results_v11.box.map:.4f}\n",
    "  Precision    : {val_results_v11.box.mp:.4f}\n",
    "  Recall       : {val_results_v11.box.mr:.4f}\n",
    "  Training Time: {yolov11_training_time/60:.2f} minutes\n",
    "\n",
    "YOLOV12 PERFORMANCE:\n",
    "  mAP@0.5      : {val_results_v12.box.map50:.4f}\n",
    "  mAP@0.5:0.95 : {val_results_v12.box.map:.4f}\n",
    "  Precision    : {val_results_v12.box.mp:.4f}\n",
    "  Recall       : {val_results_v12.box.mr:.4f}\n",
    "  Training Time: {yolov12_training_time/60:.2f} minutes\n",
    "\n",
    "PERFORMANCE DIFFERENCE:\n",
    "  mAP@0.5 Difference: {(val_results_v12.box.map50 - val_results_v11.box.map50):+.4f} ({'higher' if val_results_v12.box.map50 > val_results_v11.box.map50 else 'lower'} for YOLOv12)\n",
    "  mAP@0.5:0.95 Difference: {(val_results_v12.box.map - val_results_v11.box.map):+.4f} ({'higher' if val_results_v12.box.map > val_results_v11.box.map else 'lower'} for YOLOv12)\n",
    "  Training Time Difference: {abs(yolov12_training_time - yolov11_training_time)/60:.2f} minutes ({'faster' if yolov12_training_time < yolov11_training_time else 'slower'} for YOLOv12)\n",
    "\n",
    "TOTAL EXECUTION TIME: {total_execution_time/60:.2f} minutes\n",
    "DATE: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "{'='*80}\n",
    "\"\"\"\n",
    "\n",
    "print(summary)\n",
    "\n",
    "# Save summary\n",
    "with open(f'{RESULTS_DIR}/colab_execution_summary.txt', 'w') as f:\n",
    "    f.write(summary)\n",
    "\n",
    "print(f\"\\n Saved: {RESULTS_DIR}/colab_execution_summary.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "copy-to-drive"
   },
   "outputs": [],
   "source": [
    "# Copy all results to Google Drive\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COPYING RESULTS TO GOOGLE DRIVE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "try:\n",
    "    # Copy results directory\n",
    "    if os.path.exists(RESULTS_DIR):\n",
    "        drive_results_path = os.path.join(DRIVE_RESULTS, f\"results_{datetime.now().strftime('%Y%m%d_%H%M%S')}\")\n",
    "        shutil.copytree(RESULTS_DIR, drive_results_path)\n",
    "        print(f\" Results copied to: {drive_results_path}\")\n",
    "    \n",
    "    # Copy trained models\n",
    "    models_drive_path = os.path.join(DRIVE_RESULTS, 'models')\n",
    "    os.makedirs(models_drive_path, exist_ok=True)\n",
    "    \n",
    "    v11_model_path = '/content/runs/detect/yolov11_car_detection/weights/best.pt'\n",
    "    v12_model_path = '/content/runs/detect/yolov12_car_detection/weights/best.pt'\n",
    "    \n",
    "    if os.path.exists(v11_model_path):\n",
    "        shutil.copy(v11_model_path, os.path.join(models_drive_path, 'yolov11_best.pt'))\n",
    "        print(\" YOLOv11 model saved to Drive\")\n",
    "    \n",
    "    if os.path.exists(v12_model_path):\n",
    "        shutil.copy(v12_model_path, os.path.join(models_drive_path, 'yolov12_best.pt'))\n",
    "        print(\" YOLOv12 model saved to Drive\")\n",
    "    \n",
    "    print(f\"\\n All results saved to Google Drive: {DRIVE_RESULTS}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n  Error copying to Drive: {e}\")\n",
    "    print(\"Results are still available in /content/results/\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "final-summary"
   },
   "outputs": [],
   "source": [
    "# Final summary of generated files\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"GENERATED FILES SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nLocal Results Directory: {RESULTS_DIR}/\")\n",
    "print(f\"Google Drive Backup: {DRIVE_RESULTS}/\")\n",
    "print(\"\\nVisualizations:\")\n",
    "print(\"   eda_class_distribution.png\")\n",
    "print(\"   sample_annotated_images.png\")\n",
    "print(\"   performance_comparison_colab.png\")\n",
    "print(\"   confusion_matrices_comparison.png\")\n",
    "print(\"   inference_comparison_*.png\")\n",
    "print(\"\\nData Files:\")\n",
    "print(\"   model_comparison_colab.csv\")\n",
    "print(\"   colab_execution_summary.txt\")\n",
    "print(\"\\nTrained Models:\")\n",
    "print(\"   /content/runs/detect/yolov11_car_detection/weights/best.pt\")\n",
    "print(\"   /content/runs/detect/yolov12_car_detection/weights/best.pt\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" ALL TASKS COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nTotal execution time: {total_execution_time/60:.2f} minutes\")\n",
    "print(f\"Completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
